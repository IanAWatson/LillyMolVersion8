# iwsplit
This tool is very similar to Linux standard tool 'split'. Over time, 'split'
has evolved to become more and more similar to 'iwsplit', and today
the differences are minor.

'iwsplit' works well with the kinds of files generate by LillyMol utilities,
with knowledge about things like .sdf files and fingerprint files. 

The usage message is
```
Splits a file into chunks based on regular expressions
 -n <number>    items per chunk
 -nc <number>   number of chunks to create - will count the file to
                determine the required number of items per chunk
 -size          split into equally sized chunks - based on byte count
 -tdt           short-hand for TDT files (like .gfp)
 -sdf           short-hand for SDF files
 -rx <rx>       the regular expression to use
                SD  files need '^\$' - use the -sdf option.
                TDT files need '^\|' - use the -tdt option.
 -s             chunks start with <rx> matched records rather than end (SDF and TDT)
 -g <col>       items are defined by identifiers in column <col>
 -g sep=x       column separator for the -g column
 -stem <name>   create files with stem <name>
 -suffix <xx>   add <xx> to each file created
 -ssuffix       create chunks with same suffix as input
 -stop <number> create only <number> files
 -append        append to an existing set of split files
 -j <number>    repeat first <number> records from input in each file created
                useful for splitting descriptor files
 -stdout        also write input stream to stdout
 -dd <fname>    do NOT do any splitting, just write dd commands to <fname>
 -w <n>         width for sequence number field
 -w char=x      character to use for filling to the -w width
 -v             verbose output
```

A very common task is to split a smiles file into chunks, say 100k each.
```
iwsplit -n 100000 -suffix smi file.smi
```
generates 'iwsplit1.smi', 'iwsplit2.sm', ... each containing 100000 records.
The last file may contain fewer.

The `-nc` option works differently. The entire file must be read first in
order to determine the number of records present - so reading from standard
input will not work. Once the number of records in the input is known,
it can be written into the desired number of chunks.

The `-size` option is interesting. We have often dealt with smiles files
that have been sorted by atom count. If split into equal sized chunks,
parallel processing these files may result in the files containing smaller
molecules completing much faster than the files containing larger molecules.
The `-size` option attempts to create splits that are roughly the same
size in bytes - regardless of number of lines in each.

One of the key differences with Linux split is that the names of the files
are numbered without a fixed width field, so any number of files can be
generated - and it seems generally easier for another script to figure
out a previously generated sequence of files.

## dopattern
dopattern know about sharded files generated by iwsplit.
```
iwsplit -suffix smi -n 100000 file.smi
dopattern.sh -stem iwsplit%.smi wc iwsplit%.smi
```
dopattern examples all files of the form 'iwsplit1.smi', 'iwsplit2.smi'... in
order to determine how many files of that form are in the current directory,
and then performs the requested command on each. The magic is the '%'
character interpolation [dopattern](/docs/General/dopattern.md).
