# QSAR Models
LillyMol comes with some QSAR model building functionality.

Currently two model types are available in easy to use forms. Since
model scoring is via command line tools, scalability across cluster
and parallel computing environments is straightforward. Model evaluation
speed can be very fast, leading to very high throughput scoring.

The model types included here are all molecular descriptor models
that make use of the [make_descriptors](/docs/Molecule_Tools/make_descriptors.md)
script to convert molecules to descriptors. That script supports a variety
of 2D and 3D descriptor sets, computed using LillyMol executables.

Both building and scoring models requires molecular descriptors computed by that
script.

Note that the molecular descriptor based models described here are not necessarily
the most accurate models that can be built with LillyMol. SVM Fingerprint models
are generally more accurate, but are not fully included with this distribution.

The descriptor based models can offer significant advantages in terms of scoring
speed, and sometimes their performance is comparable to an SVMFP model.

These model building tools read their input data as Pandas DataFrames, so
make sure that your python environment includes both that, scikit-learn, and xgboost.

## Required Files

In order to build and score a model, at least two files are needed.

* Molecular Descriptors (x)
* Response (y)

Both files should be tabular, the identifier in column 1, space
separated and with a header.  Use tcount to check that files are
tabular.

For example and activity file might look like
```
Id Activity
1 2.6
2 4.1
3 -4
```

where id '1' has activity 2.6, etc...

A conformant molecular descriptor file can be generated by make_descriptors.

## XGBOOST
In the [contrib/bin/xgbd](../../contrib/bin/xgbd) directory there
are tools that enable building and scoring an XGBoost model built on molecular
descriptors.

### Model Building
The first step is to build a model. A minimal model, using a quick-to-compute
set of descriptors and taking all default XGBoost parameters, can be done via
```
${LILLYMOL_HOME}/contrib/bin/make_descriptors.sh -w train.smi > train.w
${LILLYMOL_HOME}/contrib/bin/xgbd_make.sh --mdir <mdir>
                --activity train.activity train.w
```

Suggest adding `${LILLYMOL_HOME}/contrib/bin` to your PATH.

The --mdir option specifies a directory into which information needed to evaluate
the model is stored.

Note that the script assumes that the descriptor file has been generated by
`make_descriptors` and stores the features needed with the model. Therefore
subsequent model evaluation can be done directly from a smiles file.

### Options
XGBoost has a wide variety of hyperparameters that can influence the
learning process [xgboost](https://xgboost.readthedocs.io/en/stable/parameter.html).
The defaults in the script have been found to be generally
good when dealing with a variety of SAQR datasets.
Many of these hyperparameters can be set either by command line options, or as
a textproto file specifying values for the [proto](/src//xgboost/xgboost_model.proto)
and providing that file to the script via the --proto option.

## Scoring
Scoring a model built with xgbd_make can be done in several ways. The most straightforward
is to use xgbd_evaluate

```
${LILLYMOL_HOME}/contrib/bin/xgbd_evaluate.sh -mdir <mdir>
                test.smi > test.pred
```

Note that the building process has recorded the make_descriptors descriptor sets
used during training and will call make_descriptors in order to score the
test set.

Note too that the underlying evaluation script is ruby, which calls a LillyMol
C++ executable for scoring.

One slightly unfortunate usability feature is that xgbd_make is a python script
that uses absl for argument parsing, so -- type options are recognised. The
evaluation tool is ruby, and recognises - type options. So building is done with
`--mdir` and scoring via `-mdir`.

Scoring is typically fast since it is via a C++ interface to xgboost, scoring
speed is usually the same speed as descriptor computation, and can read from
a pipe, so an unlimited number of molecules can be scored.
```
generate_smiles ... | xgbd_evaluate.sh -mdir MODEL -smi -
```

although this only works well for a single descriptor computation. There are
options in some tools to allow pipelined evaluation which does enable
fully pipelined scoring of arbitrarily large sets of molecules with multiple
descriptors.

Model performance can be evaluated via
```
iwstats -E test.activity -p 2 test.pred
```

or using any tool that can assess model performance.

## Random Forest
Very similar to xgbd_make.sh and xgbd_evaluate.sh the scripts rf_make.sh
and rf_evaluate.sh build and evaluate random forest models. The options
and functionality are very similar to the xgboost equivalents. Hyperparameters
can be specified via a [random_forest_model](/src/xgboost/random_forest_model.proto]
textproto.

Generally we find that Random Forest models are slower to build, slower to score
and usually less accurate than xgboost models.

## Other Model Types
Given the framework established with XGBoost and Random Forest models, it
is relatively straightforward to enable other model building algorithms.
Having a consistent interface makes integration with distributed evaluation
and processing straightforward.

## PostProcessing
Model outputs are often subject to filtering, often via a threshold. While
the evaluation scripts do not have built-in filtering capabilities, that
can be achieved by measures like

```
xgbd_evaluate.sh -mdir MODEL test.smi | dfilefilter -e 'response>2.0' - > passed.txt
```
